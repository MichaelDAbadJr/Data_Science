{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e4c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of x is  1.8333333333333333\n",
      "The mean of y is  2.1666666666666665\n",
      "Slope beta =  0.8157894736842105\n",
      "Intercept alpha =  0.6710526315789473\n",
      "\n",
      "Calculated by skylearn...\n",
      "Slope beta =  [[0.81578947]]\n",
      "Intercept alpha =  [0.67105263]\n",
      "\n",
      "We do the linear regression on three points (0.5, 1), (2, 2.5), and (3, 3). Please\n",
      "calculate the SSEs of the four following linear regression based on the definition SSE\n",
      "Write the Python code to output the major steps of the calculation and results\n",
      "\n",
      "a) y = x + 0.5\n",
      "\tSSE = (1-(0.5+0.5))²+(2.5-(2+0.5))²+(3-(3+0.5))²\n",
      "\tSSE =  0.25\n",
      "b) y = x + 1\n",
      "\tSSE = (1-(0.5+1))²+(2.5-(2+1))²+(3-(3+1))²\n",
      "\tSSE =  1.5\n",
      "c) y = 0.8*x + 0.3\n",
      "\tSSE = (1-(0.8*0.5+0.3))²+(2.5-(0.8*2+0.3))²+(3-(0.8*3+0.3))²\n",
      "\tSSE =  0.5399999999999997\n",
      "d) y = 0.8*x + 0.7\n",
      "\tSSE = (1-(0.8*0.5+0.7))²+(2.5-(0.8*2+0.7))²+(3-(0.8*3+0.7))²\n",
      "\tSSE =  0.06000000000000019 \n",
      "\n",
      "** (d) y = 0.8*x+0.7 is the best linear regression using SSE **\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "\n",
    "##%% Question 1: Linear regression\n",
    "x = np.array([0.5,2,3])\n",
    "y = np.array([1,2.5,3])\n",
    "\n",
    "#x\n",
    "x_mean = np.mean(x)\n",
    "print(\"The mean of x is \", x_mean)\n",
    "y_mean = np.mean(y)\n",
    "print(\"The mean of y is \", y_mean)\n",
    "\n",
    "n = (x - x_mean).dot(y - y_mean)\n",
    "d = (x - x_mean).dot(x - x_mean)\n",
    "\n",
    "beta = n / d\n",
    "alpha = y_mean - beta * x_mean\n",
    "\n",
    "print(\"Slope beta = \", beta)\n",
    "print(\"Intercept alpha = \", alpha)\n",
    "\n",
    "print(\"\\nCalculated by skylearn...\")\n",
    "# plt.scatter(x,y)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
    "print(\"Slope beta = \", model.coef_)\n",
    "print(\"Intercept alpha = \", model.intercept_)\n",
    "print(\"\\nWe do the linear regression on three points (0.5, 1), (2, 2.5), and (3, 3). Please\")\n",
    "print(\"calculate the SSEs of the four following linear regression based on the definition SSE\")\n",
    "print(\"Write the Python code to output the major steps of the calculation and results\\n\")\n",
    "squared = (\"²\")\n",
    "print(\"a) y = x + 0.5\")\n",
    "SSEa = (1-(0.5+0.5))**2+(2.5-(2+0.5))**2+(3-(3+0.5))**2\n",
    "print(\"\\tSSE = (1-(0.5+0.5)){}+(2.5-(2+0.5)){}+(3-(3+0.5)){}\".format(squared,squared,squared))\n",
    "print(\"\\tSSE = \", SSEa)\n",
    "print(\"b) y = x + 1\")\n",
    "SSEb = (1-(0.5+1))**2+(2.5-(2+1))**2+(3-(3+1))**2\n",
    "print(\"\\tSSE = (1-(0.5+1)){}+(2.5-(2+1)){}+(3-(3+1)){}\".format(squared,squared,squared))\n",
    "print(\"\\tSSE = \", SSEb)\n",
    "print(\"c) y = 0.8*x + 0.3\")\n",
    "SSEc = (1-(0.8*0.5+0.3))**2+(2.5-(0.8*2+0.3))**2+(3-(0.8*3+0.3))**2\n",
    "print(\"\\tSSE = (1-(0.8*0.5+0.3)){}+(2.5-(0.8*2+0.3)){}+(3-(0.8*3+0.3)){}\".format(squared,squared,squared))\n",
    "print(\"\\tSSE = \", SSEc)\n",
    "print(\"d) y = 0.8*x + 0.7\")\n",
    "SSEd = (1-(0.8*0.5+0.7))**2+(2.5-(0.8*2+0.7))**2+(3-(0.8*3+0.7))**2\n",
    "print(\"\\tSSE = (1-(0.8*0.5+0.7)){}+(2.5-(0.8*2+0.7)){}+(3-(0.8*3+0.7)){}\".format(squared,squared,squared))\n",
    "print(\"\\tSSE = \", SSEd,\"\\n\")\n",
    "\n",
    "print(\"** (d) y = 0.8*x+0.7 is the best linear regression using SSE **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1608eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the problem solved by Lasso and Ridge regression? What is the major\n",
      "difference between the two regression? Please discuss the advantages and disadvantages\n",
      "of them\n",
      "\n",
      "\tThe problem solved by Lasso and Ridge regression is that they apply simple techniques\n",
      "\tto prevent overfitting in linear regression. Lasso regression excludes useless variables\n",
      "\tby forcing their coefficients to be zero. Ridge regression shrinks the coefficients and\n",
      "\tit helps to reduce the model complexity and multi-collinearity\n",
      "\n",
      "\tThe superficial difference is that Ridge regression squares the variables and \n",
      "\tLasso regression takes the absolute value\n",
      "\n",
      "\tThe advantages of Ridge regression is the smaller cost results in a smaller slope\n",
      "\tand the line is less sensitie to weight than the Linear regression. The penalty\n",
      "\tterm reduces overfitting and ensures a solution.\n",
      "\tThe disadvantages is that it only shrinks the coefficients but never make them zero.\n",
      "\tTherefore it will still take some useless factors into calculation, although small.\n",
      "\n",
      "\tThe advantages of Lasso regression is that by forcing coefficients to be zero that\n",
      "\tare useless it is a very good for accuracy and reducing variance.\n",
      "\tThe disadvantage is that ridge regression tends to do a little better when most\n",
      "\tvariables are useful.\n"
     ]
    }
   ],
   "source": [
    "##%% Question 2: Lasso and Ridge regression\\\n",
    "print(\"What is the problem solved by Lasso and Ridge regression? What is the major\")\n",
    "print(\"difference between the two regression? Please discuss the advantages and disadvantages\")\n",
    "print(\"of them\\n\")\n",
    "print(\"\\tThe problem solved by Lasso and Ridge regression is that they apply simple techniques\")\n",
    "print(\"\\tto prevent overfitting in linear regression. Lasso regression excludes useless variables\")\n",
    "print(\"\\tby forcing their coefficients to be zero. Ridge regression shrinks the coefficients and\")\n",
    "print(\"\\tit helps to reduce the model complexity and multi-collinearity\\n\")\n",
    "print(\"\\tThe superficial difference is that Ridge regression squares the variables and \")\n",
    "print(\"\\tLasso regression takes the absolute value\\n\")\n",
    "print(\"\\tThe advantages of Ridge regression is the smaller cost results in a smaller slope\")\n",
    "print(\"\\tand the line is less sensitie to weight than the Linear regression. The penalty\")\n",
    "print(\"\\tterm reduces overfitting and ensures a solution.\")\n",
    "print(\"\\tThe disadvantages is that it only shrinks the coefficients but never make them zero.\")\n",
    "print(\"\\tTherefore it will still take some useless factors into calculation, although small.\\n\")\n",
    "print(\"\\tThe advantages of Lasso regression is that by forcing coefficients to be zero that\")\n",
    "print(\"\\tare useless it is a very good for accuracy and reducing variance.\")\n",
    "print(\"\\tThe disadvantage is that ridge regression tends to do a little better when most\")\n",
    "print(\"\\tvariables are useful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031cb099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. Node impurities:\n",
      "Gini index    : a-left:0.375; right:0.105; b-left:0.444; right:0.444; c-left:0.172; right:0.198;\n",
      "Entropy       : a-left:0.811; right:0.310; b-left:0.918; right:0.918; c-left:0.454; right:0.503;\n",
      "Misclass error: a-left:0.250; right:0.055; b-left:0.333; right:0.333; c-left:0.095; right:0.111;\n",
      "\n",
      "b. Qualities of Splitting:\n",
      "Gini index    : a:0.213; b:0.444; c:0.180; c is the best\n",
      "Entropy       : a:0.410; b:0.000; c:0.449; c is the best\n",
      "Misclass error: a:0.200; b:0.000; c:0.233; c is the best\n",
      "\n",
      "c. Best splitting\n",
      "Gini index    : c\n",
      "Entropy       : c\n",
      "Misclass error: c\n",
      "\n",
      "d. All three methods have the same best spitting.\n"
     ]
    }
   ],
   "source": [
    "##%% Question 3: Decision tree\n",
    "print(\"a. Node impurities:\")\n",
    "print(\"Gini index    : a-left:0.375; right:0.105; b-left:0.444; right:0.444; c-left:0.172; right:0.198;\")\n",
    "print(\"Entropy       : a-left:0.811; right:0.310; b-left:0.918; right:0.918; c-left:0.454; right:0.503;\")\n",
    "print(\"Misclass error: a-left:0.250; right:0.055; b-left:0.333; right:0.333; c-left:0.095; right:0.111;\\n\")\n",
    "print(\"b. Qualities of Splitting:\")\n",
    "print(\"Gini index    : a:0.213; b:0.444; c:0.180; c is the best\")\n",
    "print(\"Entropy       : a:0.410; b:0.000; c:0.449; c is the best\")\n",
    "print(\"Misclass error: a:0.200; b:0.000; c:0.233; c is the best\\n\")\n",
    "print(\"c. Best splitting\")\n",
    "print(\"Gini index    : c\")\n",
    "print(\"Entropy       : c\")\n",
    "print(\"Misclass error: c\\n\")\n",
    "print(\"d. All three methods have the same best spitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa78a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Iris-versicolor ID => 0\n",
      "Class Iris-setosa ID => 1\n",
      "Class Iris-virginica ID => 2\n",
      "Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]\n",
      "Mean Accuracy: 96.667%\n",
      "\n",
      "Training Error at parent node = 10/30\n",
      "Pessimistic error at parent node = (10 + 0.5)/30 = 10.5/30\n",
      "\n",
      "Training Error at leaf nodes = 9/30\n",
      "Pessimistic error at leaf nodes = (9 + 4 * 0.5)/30 = 11/30\n",
      "\n",
      "(11/30) > (10.5/30).\n",
      "\n",
      "The tree should be pruned.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2##%% Question 4: KNN\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "        print('Class %s ID => %d' % (value, i))\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\n",
    "    distance = 0.0\n",
    "    ### your code starts\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    ### your code ends\n",
    "    return sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors and return the list of neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "    ### your code starts\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    ### your code ends\n",
    "    \n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "      neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    ### your code starts\n",
    "\t# get all neighbors and make the prediction based on majority of neighbors\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_vals = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_vals), key=output_vals.count)\n",
    "    ### your code ends\n",
    "    return prediction\n",
    "\n",
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "# Test the kNN on the Iris Flowers dataset\n",
    "seed(1)\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "# the output is\n",
    "\n",
    "#The output scores is 96.66666666666667\n",
    "#The mean accuracy is 96.667%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc89e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Error at parent node = 10/30\n",
      "Pessimistic error at parent node = (10 + 0.5)/30 = 10.5/30\n",
      "\n",
      "Training Error at leaf nodes = 9/30\n",
      "Pessimistic error at leaf nodes = (9 + 4 * 0.5)/30 = 11/30\n",
      "\n",
      "(11/30) > (10.5/30).\n",
      "\n",
      "The tree should be pruned.\n"
     ]
    }
   ],
   "source": [
    "##%% Question 5: Pruning of decision tree\n",
    "print (\"\"\"\n",
    "Training Error at parent node = 10/30\n",
    "Pessimistic error at parent node = (10 + 0.5)/30 = 10.5/30\n",
    "\n",
    "Training Error at leaf nodes = 9/30\n",
    "Pessimistic error at leaf nodes = (9 + 4 * 0.5)/30 = 11/30\n",
    "\n",
    "(11/30) > (10.5/30).\n",
    "\n",
    "The tree should be pruned.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4629c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
